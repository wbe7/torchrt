# TorchRT

## Начало работы

### 1. Настройка виртуального окружения

Для изоляции зависимостей проекта используется виртуальное окружение Python. Создайте и активируйте его следующими командами:

```bash
python3 -m venv .venv
source .venv/bin/activate
```

### 2. Установка зависимостей

Мы используем `pip-tools` для управления зависимостями и обеспечения воспроизводимости окружения.

```bash
# Устанавливаем pip-tools, если его еще нет
pip install pip-tools

# Синхронизируем окружение в точном соответствии с requirements.txt
pip-sync requirements.txt

# Устанавливаем наш проект в режиме редактирования (опционально)
pip install -e .
```

#### Управление зависимостями (для разработчиков)

Если вам нужно добавить или обновить зависимость:
1. Добавьте имя пакета в `requirements.in`.
2. Запустите `pip-compile --upgrade` для перегенерации `requirements.txt`.
3. Запустите `pip-sync` для обновления вашего локального окружения.

## 3. Конвертация модели в TensorRT (с помощью Kubernetes)

Оптимизация модели в формат TensorRT (`.plan`) происходит на узле с NVIDIA GPU с помощью `Job` в Kubernetes.

### Рабочий процесс

1.  **Локальная машина**:
    *   Обучите модель, запустив `train.py`. Это создаст/обновит ONNX-модель в папке `models/`.
    *   Закоммитьте и отправьте в Git репозиторий все изменения, включая новую ONNX-модель и пустую директорию для TRT-модели (`models/california_housing_trt/1/.keep`).

2.  **Машина с доступом к Kubernetes (GPU узел)**:
    *   Склонируйте или обновите репозиторий до последней версии (`git pull`).
    *   Убедитесь, что путь в `trt-compiler-job.yaml` в секции `hostPath` соответствует абсолютному пути к папке `models` на этой машине.
    *   Запустите `Job` для конвертации:
        ```bash
        kubectl apply -f trt-compiler-job.yaml
        ```
    *   Дождитесь завершения `Job`'а. После этого в директории `models/california_housing_trt/1/` появится файл `model.plan`.
    *   Закоммитьте и отправьте в Git репозиторий новый файл `model.plan`.

3.  **Локальная машина**:
    *   Получите последнюю версию репозитория (`git pull`), чтобы скачать себе актуальный `model.plan`.

## 4. Загрузка моделей в S3

Для того чтобы KServe мог использовать модели, их необходимо загрузить в S3-совместимое хранилище. Мы используем клиент MinIO (`mc`).

### Настройка (одноразово)

1.  **Установите `mc`:**
    ```bash
    brew install minio/stable/mc
    ```
2.  **Настройте профиль `home-s3`:**
    ```bash
    mc alias set home-s3 http://192.168.77.7:9000 <ВАШ_ACCESS_KEY> <ВАШ_SECRET_KEY>
    ```
    *(Замените `<ВАШ_ACCESS_KEY>` и `<ВАШ_SECRET_KEY>` на актуальные учетные данные)*

### Синхронизация моделей

Эта команда рекурсивно копирует все модели из локальной папки `models` в бакет, сохраняя структуру директорий. Команда идемпотентна.

```bash
mc cp --recursive models/ home-s3/models/torchrt/models/
```

## 5. Развертывание InferenceService

Для развертывания моделей в KServe используется ресурс `InferenceService`. Чтобы он мог получить доступ к моделям из приватного S3-хранилища, необходимо выполнить несколько шагов, которые описаны в манифесте `inferenceservice-full.yaml`.

**Концепция:**
1.  **Создается `Secret`**, который содержит ключи доступа к S3 и аннотации с адресом эндпоинта.
2.  **Создается `ServiceAccount`**, который "использует" этот секрет.
3.  **Создается `InferenceService`**, который ссылается на этот `ServiceAccount`, получая таким образом права на скачивание моделей.

Все эти ресурсы должны находиться **в одном неймспейсе**.

### Применение манифеста

> **Важно:** Перед применением манифеста `inferenceservice-full.yaml` необходимо вручную заменить плейсхолдеры `<YOUR_ACCESS_KEY_ID>` и `<YOUR_SECRET_ACCESS_KEY>` на ваши реальные учетные данные.

После замены плейсхолдеров, примените манифест:
```bash
kubectl apply -f inferenceservice-full.yaml
```